{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087639cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets seqeval accelerate gliner -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8706f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from gliner import GLiNER\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Load the Dataset\n",
    "# This dataset is designed for text generation, so we will just use the 'note' column.\n",
    "dataset = load_dataset(\"AGBonnet/augmented-clinical-notes\", split=\"train\")\n",
    "\n",
    "# 2. Initialize the \"Teacher\" Model (GLiNER)\n",
    "# This model is smart enough to find entities without training\n",
    "teacher_model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")\n",
    "\n",
    "# 3. Define what we want to find\n",
    "labels_to_extract = [\"symptom\", \"disease\", \"medication\"]\n",
    "\n",
    "# 4. Create a \"Synthetic\" NER Dataset\n",
    "# We will process 500 samples (enough for a hackathon demo)\n",
    "tagged_samples = []\n",
    "texts = dataset.select(range(500))[\"note\"] # We use the 'note' column\n",
    "\n",
    "print(\"Auto-labeling data... this might take 2-3 minutes on GPU.\")\n",
    "\n",
    "for text in tqdm(texts):\n",
    "    # The Teacher finds the entities\n",
    "    entities = teacher_model.predict_entities(text, labels_to_extract)\n",
    "    \n",
    "    # Store in a format we can use for BioBERT\n",
    "    tagged_samples.append({\n",
    "        \"text\": text,\n",
    "        \"entities\": entities  # List of {'text': 'chest pain', 'label': 'symptom', ...}\n",
    "    })\n",
    "\n",
    "print(f\"Successfully labeled {len(tagged_samples)} notes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42697745",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "model_checkpoint = \"dmis-lab/biobert-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def create_bio_labels_from_gliner(sample):\n",
    "    text = sample[\"text\"]\n",
    "    predicted_entities = sample[\"entities\"]\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokenized_input = tokenizer(text, truncation=True, padding=\"max_length\", max_length=512, return_offsets_mapping=True)\n",
    "    \n",
    "    # Create blank labels (0 = Outside)\n",
    "    labels = [0] * len(tokenized_input[\"input_ids\"])\n",
    "    offsets = tokenized_input[\"offset_mapping\"]\n",
    "    \n",
    "    # Map GLiNER entities to BioBERT tokens\n",
    "    for ent in predicted_entities:\n",
    "        start_char = ent[\"start\"]\n",
    "        end_char = ent[\"end\"]\n",
    "        \n",
    "        # Find which tokens correspond to this character range\n",
    "        entity_start_token = None\n",
    "        entity_end_token = None\n",
    "        \n",
    "        for idx, (o_start, o_end) in enumerate(offsets):\n",
    "            if o_start == 0 and o_end == 0: continue # Skip special tokens\n",
    "            \n",
    "            if o_start == start_char:\n",
    "                entity_start_token = idx\n",
    "            if o_end == end_char:\n",
    "                entity_end_token = idx\n",
    "        \n",
    "        # If exact match found, mark it\n",
    "        if entity_start_token is not None:\n",
    "            labels[entity_start_token] = 1 # B-Entity\n",
    "            # Mark inside tokens\n",
    "            # (This is a simplified logic for hackathons; assumes entities are short)\n",
    "            if entity_end_token:\n",
    "                for i in range(entity_start_token + 1, entity_end_token + 1):\n",
    "                    labels[i] = 2 # I-Entity\n",
    "            else:\n",
    "                # Fallback for multi-token entities not perfectly aligned\n",
    "                pass \n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_input[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_input[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Convert our list to a Hugging Face Dataset object\n",
    "from datasets import Dataset\n",
    "synthetic_dataset = Dataset.from_list(tagged_samples)\n",
    "final_dataset = synthetic_dataset.map(create_bio_labels_from_gliner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd67bea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "# 1. Disable WANDB\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# 2. Setup Model\n",
    "id2label = {0: \"O\", 1: \"B-ENTITY\", 2: \"I-ENTITY\"}\n",
    "label2id = {\"O\": 0, \"B-ENTITY\": 1, \"I-ENTITY\": 2}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=3, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "# 3. Training Args (UPDATED)\n",
    "args = TrainingArguments(\n",
    "    \"biobert-gliner-finetuned\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=5e-5,             # Slightly higher LR to help it learn faster\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=10,            # <--- CHANGED FROM 1 TO 10\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=False\n",
    ")\n",
    "\n",
    "# Re-initialize to reset weights\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=final_dataset,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e6188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the trained model from memory\n",
    "classifier = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Test on a medical sentence\n",
    "text = \"Patient denies fever but complains of severe migraine and nausea.\"\n",
    "results = classifier(text)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Phase 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a411e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# STEP 1: LOAD & CLEAN DATA (FIXED)\n",
    "# ==========================================\n",
    "print(\"Loading and Cleaning Synthea Data...\")\n",
    "\n",
    "conditions = pd.read_csv(\"conditions.csv\")\n",
    "observations = pd.read_csv(\"observations.csv\")\n",
    "\n",
    "# Rename columns\n",
    "conditions = conditions.rename(columns={'DESCRIPTION': 'CONDITION', 'PATIENT': 'PATIENT_ID'})\n",
    "observations = observations.rename(columns={'DESCRIPTION': 'TEST_NAME', 'PATIENT': 'PATIENT_ID'})\n",
    "\n",
    "# --- RULE 1: REMOVE QUESTIONS ---\n",
    "# Added 'na=False' to prevent crash on empty rows\n",
    "# Added 'r' to fix SyntaxWarning\n",
    "observations = observations[~observations['TEST_NAME'].str.contains(r'\\?', regex=True, na=False)]\n",
    "\n",
    "# --- RULE 2: LENGTH FILTER ---\n",
    "# Remove anything longer than 50 characters (surveys/questions)\n",
    "observations = observations[observations['TEST_NAME'].str.len() < 50]\n",
    "\n",
    "# --- RULE 3: THE \"LAB LOOK\" FILTER ---\n",
    "# We keep rows that contain typical lab words OR standard units brackets []\n",
    "lab_keywords = [\n",
    "    'Panel', 'Blood', 'Urine', 'Glucose', 'Hemoglobin', 'Creatinine', \n",
    "    'Cholesterol', 'X-ray', 'CT', 'Scan', 'Culture', 'Count', 'Ratio', \n",
    "    'GFR', 'Triglycerides', 'Electrolytes', 'Bilirubin', 'Protein',\n",
    "    'Metabolic', 'Lipid', 'CBC', 'T3', 'T4', 'TSH'\n",
    "]\n",
    "pattern_keep = '|'.join(lab_keywords)\n",
    "\n",
    "# Filter: Must match a keyword OR have unit brackets []\n",
    "# Added 'na=False' here as well just in case\n",
    "observations = observations[\n",
    "    (observations['TEST_NAME'].str.contains(pattern_keep, case=False, na=False)) |\n",
    "    (observations['TEST_NAME'].str.contains(r'\\[', regex=True, na=False))\n",
    "]\n",
    "\n",
    "print(f\"Data Cleaned. Tracking {observations['TEST_NAME'].nunique()} unique CLINICAL tests.\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 2: TRAIN PROBABILITIES\n",
    "# ==========================================\n",
    "print(\"Training Probabilistic Model...\")\n",
    "\n",
    "merged = pd.merge(conditions[['PATIENT_ID', 'CONDITION']], \n",
    "                  observations[['PATIENT_ID', 'TEST_NAME']], \n",
    "                  on='PATIENT_ID')\n",
    "\n",
    "# Count how many times a Test happens for a Condition\n",
    "knowledge_base = merged.groupby(['CONDITION', 'TEST_NAME']).size().reset_index(name='count')\n",
    "\n",
    "# Normalize by total Condition count to get %\n",
    "condition_counts = merged['CONDITION'].value_counts().to_dict()\n",
    "knowledge_base['total_cases'] = knowledge_base['CONDITION'].map(condition_counts)\n",
    "knowledge_base['probability'] = knowledge_base['count'] / knowledge_base['total_cases']\n",
    "\n",
    "# Sort\n",
    "knowledge_base = knowledge_base.sort_values(['CONDITION', 'probability'], ascending=[True, False])\n",
    "\n",
    "print(\"Model Retrained!\")\n",
    "\n",
    "# ==========================================\n",
    "# STEP 3: INFERENCE FUNCTION\n",
    "# ==========================================\n",
    "def get_test_recommendations(symptoms_list, top_n=5):\n",
    "    recommendations = pd.DataFrame()\n",
    "    print(f\"\\nAnalyzing Symptoms: {symptoms_list}\")\n",
    "    \n",
    "    for symptom in symptoms_list:\n",
    "        matches = knowledge_base[knowledge_base['CONDITION'].str.contains(symptom, case=False, na=False)]\n",
    "        if not matches.empty:\n",
    "            recommendations = pd.concat([recommendations, matches])\n",
    "    \n",
    "    if recommendations.empty:\n",
    "        return []\n",
    "    \n",
    "    # Use MAX probability across symptoms\n",
    "    final_ranking = recommendations.groupby('TEST_NAME')['probability'].max().sort_values(ascending=False)\n",
    "    return final_ranking.head(top_n)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 4: DEMO\n",
    "# ==========================================\n",
    "print(\"\\nTop 5 Common Conditions in your Dataset:\")\n",
    "print(merged['CONDITION'].value_counts().head(5))\n",
    "# ... (Assume you have already loaded and filtered 'observations' as per the previous code)\n",
    "\n",
    "# 1. Count the total rows (The height of the table)\n",
    "final_row_count = len(observations)\n",
    "\n",
    "# 2. Count the unique tests (The number of distinct labels)\n",
    "unique_test_count = observations['TEST_NAME'].nunique()\n",
    "\n",
    "print(f\"--- FILTERING REPORT ---\")\n",
    "print(f\"Final rows (Total Volume): {final_row_count}\")\n",
    "print(f\"Unique Tests (Distinct Types): {unique_test_count}\")\n",
    "# Test 1: Diabetes\n",
    "symptoms_2 = ['diabetes']\n",
    "results_2 = get_test_recommendations(symptoms_2)\n",
    "\n",
    "print(\"\\n--- DIAGNOSTIC PATH FOR DIABETES ---\")\n",
    "for test, prob in results_2.items():\n",
    "    print(f\"[ ] {test} (Confidence: {prob:.1%})\")\n",
    "\n",
    "# Test 2: Hypertension\n",
    "symptoms_3 = ['hypertension']\n",
    "results_3 = get_test_recommendations(symptoms_3)\n",
    "\n",
    "print(\"\\n--- DIAGNOSTIC PATH FOR HYPERTENSION ---\")\n",
    "for test, prob in results_3.items():\n",
    "    print(f\"[ ] {test} (Confidence: {prob:.1%})\")\n",
    "\n",
    "symptoms_4 = ['pneumonia']\n",
    "results_4 = get_test_recommendations(symptoms_4)\n",
    "\n",
    "for test, prob in results_4.items():\n",
    "    print(f\"[ ] {test} (Confidence: {prob:.1%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
